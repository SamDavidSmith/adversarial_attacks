{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create TFRecords\n",
    "This is in order to store annotation data in specific format for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARCHIVE_FILES = os.path.join(paths['IMAGE_PATH'], 'archive.tar.gz')\n",
    "if os.path.exists(ARCHIVE_FILES):\n",
    "  !tar -zxvf {ARCHIVE_FILES}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into '/content/drive/MyDrive/Tensorflow/scripts'...\n",
      "remote: Enumerating objects: 3, done.\u001b[K\n",
      "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
      "remote: Compressing objects: 100% (2/2), done.\u001b[K\n",
      "remote: Total 3 (delta 0), reused 1 (delta 0), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (3/3), done.\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(files['TF_RECORD_SCRIPT']):\n",
    "    !git clone https://github.com/nicknochnack/GenerateTFRecord {paths['SCRIPTS_PATH']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Had to open the generate_tfrecord.py file and edit some code, specifically the function that parses and extracts the information from the .xml files. The problem I was encountering comes from the length of the < object > class if you download from Roboflow is slightly longer than the .xml files created manually with labelImg. The function below is able to handle these differences by measuring the length of the < object > class. For some frustrating reason, there isn't a universally respected order of the < ymin >, < ymax >, < xmin >, < xmax > objects, so some datasets have these in a different order. This function will accommodate these discrepancies.\n",
    "\n",
    "    xml_list = []\n",
    "    for xml_file in glob.glob(path + '/*.xml'):\n",
    "        tree = ET.parse(xml_file)\n",
    "        root = tree.getroot()\n",
    "        for member in root.findall('object'):\n",
    "            class_name = member[0].text\n",
    "            \n",
    "            if len(member) == 5: # Handling Roboflow XML format\n",
    "                bbox = member.find('bndbox')\n",
    "                xmin = int(bbox.find('xmin').text)\n",
    "                ymin = int(bbox.find('ymin').text)\n",
    "                xmax = int(bbox.find('xmax').text)\n",
    "                ymax = int(bbox.find('ymax').text)\n",
    "            elif len(member) == 6:  # Handling labelImg XML format\n",
    "                bbox = member.find('bndbox')\n",
    "                xmin = int(bbox.find('xmin').text)\n",
    "                ymin = int(bbox.find('ymin').text)\n",
    "                xmax = int(bbox.find('xmax').text)\n",
    "                ymax = int(bbox.find('ymax').text)\n",
    "\n",
    "            value = (root.find('filename').text,\n",
    "                     int(root.find('size')[0].text),\n",
    "                     int(root.find('size')[1].text),\n",
    "                     class_name,\n",
    "                     xmin,\n",
    "                     ymin,\n",
    "                     xmax,\n",
    "                     ymax\n",
    "                     )\n",
    "            xml_list.append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import imghdr\n",
    "from pathlib import Path\n",
    "\n",
    "data_dir = \"/content/drive/MyDrive/Tensorflow/workspace/images/train2\"\n",
    "image_extensions = [\".jpg\", \".png\", \".webp\", \".jpeg\"]\n",
    "\n",
    "for image_path in Path(data_dir).rglob(\"*\"):\n",
    "  if image_path.suffix.lower() in image_extensions:\n",
    "    img_type = imghdr.what(image_path)\n",
    "    if img_type is not None and img_type == \"webp\":\n",
    "      img = Image.open(image_path)\n",
    "      img = img.convert(\"RGB\")  # Convert to RGB mode\n",
    "      img.save(image_path.with_suffix(\".jpg\"), \"JPEG\")\n",
    "      print(f\"Converted {image_path} to JPEG\")\n",
    "\n",
    "      # Find corresponding XML annotation file\n",
    "      xml_filename = image_path.stem + \".xml\"\n",
    "      xml_path = os.path.join(data_dir, xml_filename)\n",
    "      if os.path.exists(xml_path):\n",
    "        shutil.copy(xml_path, image_path.with_suffix(\".jpg\").with_name(xml_filename))\n",
    "        print(f\"Copied {xml_path} to {image_path.with_suffix('.jpg').with_name(xml_filename)}\")\n",
    "    else:\n",
    "      img = Image.open(image_path)\n",
    "      img = img.convert(\"RGB\")  # Convert to RGB mode\n",
    "      img.save(image_path)\n",
    "      print(f\"Converting {image_path} to JPEG\")\n",
    "\n",
    "      # Find corresponding XML annotation file\n",
    "      xml_filename = image_path.stem + \".xml\"\n",
    "      xml_path = os.path.join(data_dir, xml_filename)\n",
    "      if os.path.exists(xml_path):\n",
    "        shutil.copy(xml_path, image_path.with_name(xml_filename))\n",
    "        print(f\"Copied {xml_path} to {image_path.with_name(xml_filename)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: birds, Count: 444\n",
      "Class: car, Count: 285\n",
      "Class: cliff, Count: 189\n",
      "Class: cloud, Count: 178\n",
      "Class: hay-bale, Count: 867\n",
      "Class: House, Count: 529\n",
      "Class: Lake, Count: 223\n",
      "Class: human, Count: 250\n",
      "Class: tree, Count: 173\n"
     ]
    }
   ],
   "source": [
    "train_folder = '/content/drive/MyDrive/Tensorflow/workspace/images/NEWERtrain2_converted'\n",
    "label_map = {label['name']: label['id'] for label in labels}\n",
    "class_counts = {label['name']: 0 for label in labels}\n",
    "for file in os.listdir(train_folder):\n",
    "  if file.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "    for label in labels:\n",
    "      class_name = label['name']\n",
    "      if file.startswith(f\"{class_name}\"):\n",
    "        class_counts[class_name] += 1\n",
    "\n",
    "for label in labels:\n",
    "  class_name = label['name']\n",
    "  count = class_counts[class_name]\n",
    "  print(f\"Class: {class_name}, Count: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: birds, Count: 87\n",
      "Class: car, Count: 43\n",
      "Class: cliff, Count: 34\n",
      "Class: cloud, Count: 24\n",
      "Class: hay-bale, Count: 445\n",
      "Class: House, Count: 40\n",
      "Class: Lake, Count: 100\n",
      "Class: human, Count: 75\n",
      "Class: tree, Count: 35\n"
     ]
    }
   ],
   "source": [
    "train_folder = '/content/drive/MyDrive/Tensorflow/workspace/images/test2_converted'\n",
    "label_map = {label['name']: label['id'] for label in labels}\n",
    "class_counts = {label['name']: 0 for label in labels}\n",
    "for file in os.listdir(train_folder):\n",
    "  if file.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "    for label in labels:\n",
    "      class_name = label['name']\n",
    "      if file.startswith(f\"{class_name}\"):\n",
    "        class_counts[class_name] += 1\n",
    "\n",
    "for label in labels:\n",
    "  class_name = label['name']\n",
    "  count = class_counts[class_name]\n",
    "  print(f\"Class: {class_name}, Count: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content/drive/MyDrive/Tensorflow/workspace/images'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "paths['IMAGE_PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique file extensions in the directory:\n",
      "{'.xml', '.jpg'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "image_dir = \"/content/drive/MyDrive/Tensorflow/workspace/images/TOPDOWNtest\"\n",
    "extensions = set()\n",
    "\n",
    "for filename in os.listdir(image_dir):\n",
    "  file_extension = os.path.splitext(filename)[1].lower()\n",
    "  extensions.add(file_extension)\n",
    "print(\"Unique file extensions in the directory:\")\n",
    "print(extensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_dir = \"/content/drive/MyDrive/Tensorflow/workspace/images/TOPDOWNtest\"\n",
    "\n",
    "for xml_file in os.listdir(xml_dir):\n",
    "  if xml_file.endswith('.xml'):\n",
    "    xml_path = os.path.join(xml_dir, xml_file)\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    for filename_elem in root.iter('filename'):\n",
    "      new_filename = filename_elem.text.replace('.JPG', '.jpg')\n",
    "      filename_elem.text = new_filename\n",
    "\n",
    "    for path_elem in root.iter('path'):\n",
    "      new_path = path_elem.text.replace('.JPG', '.jpg')\n",
    "      path_elem.text = new_path\n",
    "\n",
    "    # Save the updated XML\n",
    "    tree.write(xml_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XML files with 'tree' object class:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "xml_dir = \"/content/drive/MyDrive/Tensorflow/workspace/images/TOPDOWNtrain\"\n",
    "xml_files_with_tree = []\n",
    "\n",
    "for xml_file in os.listdir(xml_dir):\n",
    "  if xml_file.endswith('.xml'):\n",
    "    xml_path = os.path.join(xml_dir, xml_file)\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    for obj in root.findall('object'):\n",
    "      name_elem = obj.find('name')\n",
    "        if name_elem is not None and name_elem.text == 'tree':\n",
    "          xml_files_with_tree.append(xml_file)\n",
    "          break\n",
    "\n",
    "print(\"XML files with 'tree' object class:\")\n",
    "print(xml_files_with_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_file_path = \"/content/drive/MyDrive/Tensorflow/workspace/images/TOPDOWNtrain/housetp_8.xml\"\n",
    "new_object_name = \"vegetation\"\n",
    "\n",
    "tree = ET.parse(xml_file_path)\n",
    "root = tree.getroot()\n",
    "\n",
    "for obj in root.findall('object'):\n",
    "  name_elem = obj.find('name')\n",
    "  if name_elem is not None and name_elem.text == 'tree':\n",
    "    name_elem.text = new_object_name\n",
    "    break\n",
    "\n",
    "# Save the updated XML\n",
    "tree.write(xml_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above functions are simply to check if the right image extensions exist. They also contain functions to make sure the .xml files all contain the right class names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created the TFRecord file: /content/drive/MyDrive/Tensorflow/workspace/annotations/train.record\n",
      "Successfully created the TFRecord file: /content/drive/MyDrive/Tensorflow/workspace/annotations/test.record\n"
     ]
    }
   ],
   "source": [
    "!python {files['TF_RECORD_SCRIPT']} -x {os.path.join(paths['IMAGE_PATH'], 'TOPDOWNtrain')} -l {files['LABELMAP']} -o {os.path.join(paths['ANNOTATION_PATH'], 'train.record')}\n",
    "#!python {files['TF_RECORD_SCRIPT']} -x {os.path.join(paths['IMAGE_PATH'], 'valid')} -l {files['LABELMAP']} -o {os.path.join(paths['ANNOTATION_PATH'], 'valid.record')}\n",
    "!python {files['TF_RECORD_SCRIPT']} -x {os.path.join(paths['IMAGE_PATH'], 'TOPDOWNtest')} -l {files['LABELMAP']} -o {os.path.join(paths['ANNOTATION_PATH'], 'test.record')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp {os.path.join(paths['PRETRAINED_MODEL_PATH'], PRETRAINED_MODEL_NAME, 'pipeline.config')} {os.path.join(paths['CHECKPOINT_PATH'])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure pipeline\n",
    "\n",
    "Configure pipeline of the model for my image files, label mapping, and most importantly transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.protos import pipeline_pb2\n",
    "from google.protobuf import text_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = config_util.get_configs_from_pipeline_file(files['PIPELINE_CONFIG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': ssd {\n",
       "   num_classes: 90\n",
       "   image_resizer {\n",
       "     fixed_shape_resizer {\n",
       "       height: 320\n",
       "       width: 320\n",
       "     }\n",
       "   }\n",
       "   feature_extractor {\n",
       "     type: \"ssd_mobilenet_v2_fpn_keras\"\n",
       "     depth_multiplier: 1.0\n",
       "     min_depth: 16\n",
       "     conv_hyperparams {\n",
       "       regularizer {\n",
       "         l2_regularizer {\n",
       "           weight: 3.9999998989515007e-05\n",
       "         }\n",
       "       }\n",
       "       initializer {\n",
       "         random_normal_initializer {\n",
       "           mean: 0.0\n",
       "           stddev: 0.009999999776482582\n",
       "         }\n",
       "       }\n",
       "       activation: RELU_6\n",
       "       batch_norm {\n",
       "         decay: 0.996999979019165\n",
       "         scale: true\n",
       "         epsilon: 0.0010000000474974513\n",
       "       }\n",
       "     }\n",
       "     use_depthwise: true\n",
       "     override_base_feature_extractor_hyperparams: true\n",
       "     fpn {\n",
       "       min_level: 3\n",
       "       max_level: 7\n",
       "       additional_layer_depth: 128\n",
       "     }\n",
       "   }\n",
       "   box_coder {\n",
       "     faster_rcnn_box_coder {\n",
       "       y_scale: 10.0\n",
       "       x_scale: 10.0\n",
       "       height_scale: 5.0\n",
       "       width_scale: 5.0\n",
       "     }\n",
       "   }\n",
       "   matcher {\n",
       "     argmax_matcher {\n",
       "       matched_threshold: 0.5\n",
       "       unmatched_threshold: 0.5\n",
       "       ignore_thresholds: false\n",
       "       negatives_lower_than_unmatched: true\n",
       "       force_match_for_each_row: true\n",
       "       use_matmul_gather: true\n",
       "     }\n",
       "   }\n",
       "   similarity_calculator {\n",
       "     iou_similarity {\n",
       "     }\n",
       "   }\n",
       "   box_predictor {\n",
       "     weight_shared_convolutional_box_predictor {\n",
       "       conv_hyperparams {\n",
       "         regularizer {\n",
       "           l2_regularizer {\n",
       "             weight: 3.9999998989515007e-05\n",
       "           }\n",
       "         }\n",
       "         initializer {\n",
       "           random_normal_initializer {\n",
       "             mean: 0.0\n",
       "             stddev: 0.009999999776482582\n",
       "           }\n",
       "         }\n",
       "         activation: RELU_6\n",
       "         batch_norm {\n",
       "           decay: 0.996999979019165\n",
       "           scale: true\n",
       "           epsilon: 0.0010000000474974513\n",
       "         }\n",
       "       }\n",
       "       depth: 128\n",
       "       num_layers_before_predictor: 4\n",
       "       kernel_size: 3\n",
       "       class_prediction_bias_init: -4.599999904632568\n",
       "       share_prediction_tower: true\n",
       "       use_depthwise: true\n",
       "     }\n",
       "   }\n",
       "   anchor_generator {\n",
       "     multiscale_anchor_generator {\n",
       "       min_level: 3\n",
       "       max_level: 7\n",
       "       anchor_scale: 4.0\n",
       "       aspect_ratios: 1.0\n",
       "       aspect_ratios: 2.0\n",
       "       aspect_ratios: 0.5\n",
       "       scales_per_octave: 2\n",
       "     }\n",
       "   }\n",
       "   post_processing {\n",
       "     batch_non_max_suppression {\n",
       "       score_threshold: 9.99999993922529e-09\n",
       "       iou_threshold: 0.6000000238418579\n",
       "       max_detections_per_class: 100\n",
       "       max_total_detections: 100\n",
       "       use_static_shapes: false\n",
       "     }\n",
       "     score_converter: SIGMOID\n",
       "   }\n",
       "   normalize_loss_by_num_matches: true\n",
       "   loss {\n",
       "     localization_loss {\n",
       "       weighted_smooth_l1 {\n",
       "       }\n",
       "     }\n",
       "     classification_loss {\n",
       "       weighted_sigmoid_focal {\n",
       "         gamma: 2.0\n",
       "         alpha: 0.25\n",
       "       }\n",
       "     }\n",
       "     classification_weight: 1.0\n",
       "     localization_weight: 1.0\n",
       "   }\n",
       "   encode_background_as_zeros: true\n",
       "   normalize_loc_loss_by_codesize: true\n",
       "   inplace_batchnorm_update: true\n",
       "   freeze_batchnorm: false\n",
       " },\n",
       " 'train_config': batch_size: 128\n",
       " data_augmentation_options {\n",
       "   random_horizontal_flip {\n",
       "   }\n",
       " }\n",
       " data_augmentation_options {\n",
       "   random_crop_image {\n",
       "     min_object_covered: 0.0\n",
       "     min_aspect_ratio: 0.75\n",
       "     max_aspect_ratio: 3.0\n",
       "     min_area: 0.75\n",
       "     max_area: 1.0\n",
       "     overlap_thresh: 0.0\n",
       "   }\n",
       " }\n",
       " sync_replicas: true\n",
       " optimizer {\n",
       "   momentum_optimizer {\n",
       "     learning_rate {\n",
       "       cosine_decay_learning_rate {\n",
       "         learning_rate_base: 0.07999999821186066\n",
       "         total_steps: 50000\n",
       "         warmup_learning_rate: 0.026666000485420227\n",
       "         warmup_steps: 1000\n",
       "       }\n",
       "     }\n",
       "     momentum_optimizer_value: 0.8999999761581421\n",
       "   }\n",
       "   use_moving_average: false\n",
       " }\n",
       " fine_tune_checkpoint: \"PATH_TO_BE_CONFIGURED\"\n",
       " num_steps: 50000\n",
       " startup_delay_steps: 0.0\n",
       " replicas_to_aggregate: 8\n",
       " max_number_of_boxes: 100\n",
       " unpad_groundtruth_tensors: false\n",
       " fine_tune_checkpoint_type: \"classification\"\n",
       " fine_tune_checkpoint_version: V2,\n",
       " 'train_input_config': label_map_path: \"PATH_TO_BE_CONFIGURED\"\n",
       " tf_record_input_reader {\n",
       "   input_path: \"PATH_TO_BE_CONFIGURED\"\n",
       " },\n",
       " 'eval_config': metrics_set: \"coco_detection_metrics\"\n",
       " use_moving_averages: false,\n",
       " 'eval_input_configs': [label_map_path: \"PATH_TO_BE_CONFIGURED\"\n",
       " shuffle: false\n",
       " num_epochs: 1\n",
       " tf_record_input_reader {\n",
       "   input_path: \"PATH_TO_BE_CONFIGURED\"\n",
       " }\n",
       " ],\n",
       " 'eval_input_config': label_map_path: \"PATH_TO_BE_CONFIGURED\"\n",
       " shuffle: false\n",
       " num_epochs: 1\n",
       " tf_record_input_reader {\n",
       "   input_path: \"PATH_TO_BE_CONFIGURED\"\n",
       " }}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\n",
    "with tf.io.gfile.GFile(files['PIPELINE_CONFIG'], \"r\") as f:\n",
    "    proto_str = f.read()\n",
    "    text_format.Merge(proto_str, pipeline_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/content/drive/MyDrive/Tensorflow/workspace/annotations/test.record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_config.model.ssd.num_classes = len(labels)\n",
    "pipeline_config.train_config.batch_size = 8\n",
    "pipeline_config.train_config.fine_tune_checkpoint = os.path.join(paths['PRETRAINED_MODEL_PATH'], PRETRAINED_MODEL_NAME, 'checkpoint', 'ckpt-0')\n",
    "pipeline_config.train_config.fine_tune_checkpoint_type = \"detection\"\n",
    "pipeline_config.train_input_reader.label_map_path= files['LABELMAP']\n",
    "pipeline_config.train_input_reader.tf_record_input_reader.input_path[:] = [os.path.join(paths['ANNOTATION_PATH'], 'train.record')]\n",
    "pipeline_config.eval_input_reader[0].label_map_path = files['LABELMAP']\n",
    "pipeline_config.eval_input_reader[0].tf_record_input_reader.input_path[:] = [os.path.join(paths['ANNOTATION_PATH'], 'test.record')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_text = text_format.MessageToString(pipeline_config)\n",
    "with tf.io.gfile.GFile(files['PIPELINE_CONFIG'], \"wb\") as f:\n",
    "    f.write(config_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Updated Pipeline Configuration:\")\n",
    "print(config_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XML FILES DEBUGGING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# .XML FILES DEBUGGING\n",
    "\n",
    "It can be investigated online on sites such as Stack Overflow, that .xml files on Roboflow sometimes contain dubious data, such as bounding boxes being larger than the images that contain them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like an unknown error is persisting when attempting to train the model. This could be caused by the massive amount of training images present for some classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of image files in 'train': 304\n",
      "Number of annotation files in 'train': 304\n"
     ]
    }
   ],
   "source": [
    "def count_images_and_annotations(folder_path):\n",
    "  image_count = 0\n",
    "  annotation_count = 0\n",
    "\n",
    "  for root, dirs, files in os.walk(folder_path):\n",
    "    for file in files:\n",
    "      if file.lower().endswith(('.jpg', '.jpeg', '.png', '.jfif', '.webp')):\n",
    "        image_count += 1\n",
    "      elif file.lower().endswith('.xml'):\n",
    "        annotation_count += 1\n",
    "\n",
    "  return image_count, annotation_count\n",
    "\n",
    "train_folder = '/content/drive/MyDrive/Tensorflow/workspace/images/TOPDOWNtrain'\n",
    "image_count, annotation_count = count_images_and_annotations(train_folder)\n",
    "\n",
    "print(f\"Number of image files in 'train': {image_count}\")\n",
    "print(f\"Number of annotation files in 'train': {annotation_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image files without corresponding XML annotations:\n",
      "Total number of missing annotations: 0\n"
     ]
    }
   ],
   "source": [
    "image_folder = '/content/drive/MyDrive/Tensorflow/workspace/images/TOPDOWNtest'\n",
    "missing_annotations = []\n",
    "\n",
    "image_files = [file for file in os.listdir(image_folder) if file.lower().endswith(('.jpg', '.png', '.jfif', '.jpeg'))]\n",
    "\n",
    "for image_file in image_files:\n",
    "  image_name = os.path.splitext(image_file)[0]\n",
    "  xml_file = image_name + '.xml'\n",
    "  xml_path = os.path.join(image_folder, xml_file)\n",
    "  \n",
    "  if not os.path.exists(xml_path):\n",
    "    missing_annotations.append(image_file)\n",
    "\n",
    "print(\"Image files without corresponding XML annotations:\")\n",
    "for missing_file in missing_annotations:\n",
    "  print(missing_file)\n",
    "\n",
    "print(f\"Total number of missing annotations: {len(missing_annotations)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a side note, it looks like two image files do not sport related annotation files in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted image: cloud63.jpg\n",
      "Deleted image: cloud94.jpg\n"
     ]
    }
   ],
   "source": [
    "image_folder = '/content/drive/MyDrive/Tensorflow/workspace/images/train2'\n",
    "\n",
    "images_to_delete = ['cloud63.jpg', 'cloud94.jpg']\n",
    "\n",
    "for image_to_delete in images_to_delete:\n",
    "  image_path = os.path.join(image_folder, image_to_delete)\n",
    "  if os.path.exists(image_path):\n",
    "    os.remove(image_path)\n",
    "    print(f\"Deleted image: {image_to_delete}\")\n",
    "  else:\n",
    "    print(f\"Image not found: {image_to_delete}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below function checks for any outliers in the .xml annotation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_xml_annotations(xml_folder):\n",
    "  xml_files = [f for f in os.listdir(xml_folder) if f.endswith('.xml')]\n",
    "  problematic_files = []\n",
    "  for xml_file in xml_files:\n",
    "    xml_path = os.path.join(xml_folder, xml_file)\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    width = int(root.find('size/width').text)\n",
    "    height = int(root.find('size/height').text)\n",
    "\n",
    "    for obj in root.findall('object'):\n",
    "      bbox = obj.find('bndbox')\n",
    "      xmin = int(bbox.find('xmin').text)\n",
    "      ymin = int(bbox.find('ymin').text)\n",
    "      xmax = int(bbox.find('xmax').text)\n",
    "      ymax = int(bbox.find('ymax').text)\n",
    "\n",
    "      if xmin < 0 or ymin < 0 or xmax > width or ymax > height:\n",
    "        problematic_files.append(xml_file)\n",
    "        break\n",
    "\n",
    "  return problematic_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No problematic annotations found.\n"
     ]
    }
   ],
   "source": [
    "xml_folder = '/content/drive/MyDrive/Tensorflow/workspace/images/TOPDOWNtest'\n",
    "problematic_files = check_xml_annotations(xml_folder)\n",
    "\n",
    "if len(problematic_files) == 0:\n",
    "    print(\"No problematic annotations found.\")\n",
    "else:\n",
    "    print(\"Problematic annotations found in the following files:\")\n",
    "    for file in problematic_files:\n",
    "        print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are loads of problematic .xml files which contain such issues as bounding boxes stretching from 'xmin=1' to 'xmin=641', when the image is only 640 wide, for the image below. It coould be seen this occurs in cases where the object carries on off the screen, and annotated as such, and later when the image is compressed, the pixel x-value is calculated to be off of the screen. As seen on Stack Overflow, it is necessary to delete these .xml files along with related images if you want to train the model correctly. ![8185a9c7-5b0a-4722-a336-0ee69d7d0ee4.jfif](data:image/jpeg;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will delete problematic pairs of image/xml files found in the training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_problematic_data(xml_folder, problematic_files):\n",
    "  for xml_file in problematic_files:\n",
    "    xml_path = os.path.join(xml_folder, xml_file)\n",
    "    image_path = os.path.join('/content/drive/MyDrive/Tensorflow/workspace/images/train2', xml_file.replace('.xml', '.jpg'))\n",
    "    if os.path.exists(xml_path):\n",
    "      os.remove(xml_path)\n",
    "      print(f\"Deleted XML file: {xml_file}\")\n",
    "\n",
    "    if os.path.exists(image_path):\n",
    "      os.remove(image_path)\n",
    "      print(f\"Deleted image file: {xml_file.replace('.xml', '.jpg')}\")\n",
    "\n",
    "xml_folder = '/content/drive/MyDrive/Tensorflow/workspace/images/train2'\n",
    "problematic_files = problematic_files\n",
    "\n",
    "delete_problematic_data(xml_folder, problematic_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now checking the amount of images in train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of image files in 'train': 5423\n",
      "Number of annotation files in 'train': 5423\n"
     ]
    }
   ],
   "source": [
    "def count_images_and_annotations(folder_path):\n",
    "    image_count = 0\n",
    "    annotation_count = 0\n",
    "\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.jpg', '.jpeg', '.png', '.jfif')):\n",
    "                image_count += 1\n",
    "            elif file.lower().endswith('.xml'):\n",
    "                annotation_count += 1\n",
    "\n",
    "    return image_count, annotation_count\n",
    "\n",
    "train_folder = '/content/drive/MyDrive/Tensorflow/workspace/images/train'\n",
    "image_count, annotation_count = count_images_and_annotations(train_folder)\n",
    "\n",
    "print(f\"Number of image files in 'train': {image_count}\")\n",
    "print(f\"Number of annotation files in 'train': {annotation_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are about 1,000 less images in the training set, and then after performing the same with the testing set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of image files in 'test': 1845\n",
      "Number of annotation files in 'test': 1845\n"
     ]
    }
   ],
   "source": [
    "def count_images_and_annotations(folder_path):\n",
    "    image_count = 0\n",
    "    annotation_count = 0\n",
    "\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.jpg', '.jpeg', '.png', '.jfif')):\n",
    "                image_count += 1\n",
    "            elif file.lower().endswith('.xml'):\n",
    "                annotation_count += 1\n",
    "\n",
    "    return image_count, annotation_count\n",
    "\n",
    "test_folder = '/content/drive/MyDrive/Tensorflow/workspace/images/test'\n",
    "image_count, annotation_count = count_images_and_annotations(test_folder)\n",
    "\n",
    "print(f\"Number of image files in 'test': {image_count}\")\n",
    "print(f\"Number of annotation files in 'test': {annotation_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: birds, Count: 459\n",
      "Class: car, Count: 363\n",
      "Class: cliff, Count: 343\n",
      "Class: pool, Count: 421\n",
      "Class: tree, Count: 632\n",
      "Class: cloud, Count: 815\n",
      "Class: hay-bale, Count: 3615\n",
      "Class: human, Count: 8014\n",
      "Class: House, Count: 529\n",
      "Class: Lake, Count: 220\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "train_folder = '/content/drive/MyDrive/Tensorflow/workspace/images/train/'\n",
    "class_counts = defaultdict(int)\n",
    "\n",
    "# Loop through all XML files in the train folder\n",
    "for xml_file in os.listdir(train_folder):\n",
    "  if xml_file.endswith('.xml'):\n",
    "    xml_path = os.path.join(train_folder, xml_file)\n",
    "    with open(xml_path, 'r') as f:\n",
    "      lines = f.readlines()\n",
    "      for line in lines:\n",
    "        if '<name>' in line:\n",
    "          class_name = line.strip().split('<name>')[1].split('</name>')[0]\n",
    "          class_counts[class_name] += 1\n",
    "\n",
    "# Print the counts for each object class\n",
    "for class_name, count in class_counts.items():\n",
    "  print(f\"Class: {class_name}, Count: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the number of images belonging to each class is uneven, particularly with 'human' and 'hay-bale', let's first see if the model is working after deleting these problematic files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem still persists, and after looking online some more, it appears some .xml files can also contain negative values for width and height, or have bounding boxes with area 0. The following function will go over all .xml files again and check both the training and test set for this problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_xml_annotations(xml_folder):\n",
    "  xml_files = [f for f in os.listdir(xml_folder) if f.endswith('.xml')]\n",
    "\n",
    "  problematic_files = []\n",
    "\n",
    "  for xml_file in xml_files:\n",
    "    xml_path = os.path.join(xml_folder, xml_file)\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    width = int(root.find('size/width').text)\n",
    "    height = int(root.find('size/height').text)\n",
    "\n",
    "    for obj in root.findall('object'):\n",
    "      bbox = obj.find('bndbox')\n",
    "      xmin = int(bbox.find('xmin').text)\n",
    "      ymin = int(bbox.find('ymin').text)\n",
    "      xmax = int(bbox.find('xmax').text)\n",
    "      ymax = int(bbox.find('ymax').text)\n",
    "\n",
    "      if xmin < 0 or ymin < 0 or xmax < 0 or ymax < 0 or xmin > xmax or ymin > ymax:\n",
    "        problematic_files.append(xml_file)\n",
    "        break\n",
    "\n",
    "      box_width = xmax - xmin\n",
    "      box_height = ymax - ymin\n",
    "      if box_width <= 0 or box_height <= 0:\n",
    "        problematic_files.append(xml_file)\n",
    "        break\n",
    "\n",
    "  return problematic_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No problematic annotations found.\n"
     ]
    }
   ],
   "source": [
    "xml_folder = '/content/drive/MyDrive/Tensorflow/workspace/images/TOPDOWNtest'\n",
    "problematic_files = check_xml_annotations(xml_folder)\n",
    "\n",
    "if len(problematic_files) == 0:\n",
    "    print(\"No problematic annotations found.\")\n",
    "else:\n",
    "    print(\"Problematic annotations found in the following files:\")\n",
    "    for file in problematic_files:\n",
    "        print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if this has had any effect on the command to train the Tensorflow model above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On second thought, the pre-processing function built-in to the model may be encountering issues with certain resolutions in the training folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_image_resolutions(image_folder):\n",
    "  unique_resolutions = set()\n",
    "  for file in os.listdir(image_folder):\n",
    "    if file.lower().endswith(('.jpg', '.png', '.jfif', '.jpeg')):\n",
    "      image_path = os.path.join(image_folder, file)\n",
    "        try:\n",
    "          img = Image.open(image_path)\n",
    "          width, height = img.size\n",
    "          unique_resolutions.add((width, height))\n",
    "        except Exception as e:\n",
    "          print(f\"Error processing image: {image_path}\")\n",
    "          print(f\"Error message: {str(e)}\")\n",
    "\n",
    "  return unique_resolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = '/content/drive/MyDrive/Tensorflow/workspace/images/train'\n",
    "resolutions = get_unique_image_resolutions(image_folder)\n",
    "\n",
    "print(\"Unique image resolutions:\")\n",
    "for width, height in resolutions:\n",
    "    print(f\"{width} x {height}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_folder = '/content/drive/MyDrive/Tensorflow/workspace/images/train'\n",
    "\n",
    "# Path to new folder to store selected images\n",
    "new_folder = '/content/drive/MyDrive/Tensorflow/workspace/images/train_small'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_folder = '/content/drive/MyDrive/Tensorflow/workspace/images/test'\n",
    "\n",
    "# Path to new folder to store selected images\n",
    "new_folder = '/content/drive/MyDrive/Tensorflow/workspace/images/test_small'\n",
    "if not os.path.exists(new_folder):\n",
    "    os.makedirs(new_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [{'name':'birds', 'id':1}, {'name':'car', 'id':2}, {'name':'cliff', 'id':3}, {'name':'cloud', 'id':4}, {'name':'hay-bale', 'id':5}, {'name':'House', 'id':6}, {'name':'Lake', 'id':7}, {'name':'human', 'id':8}, {'name':'pool', 'id':9}, {'name':'tree', 'id':10}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images_per_class = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images and XML files copied to new folder.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import shutil\n",
    "# Iterate through each class\n",
    "for class_info in labels:\n",
    "  class_name = class_info['name']\n",
    "\n",
    "  # Get list of image files for this class\n",
    "  class_image_files = [f for f in os.listdir(original_folder) if f.startswith(class_name) and f.endswith('.jpg')]\n",
    "\n",
    "  # Shuffle the list of image files\n",
    "  random.shuffle(class_image_files)\n",
    "\n",
    "  # Select the first num_images_per_class\n",
    "  selected_image_files = class_image_files[:num_images_per_class]\n",
    "\n",
    "  # Copy selected image files and corresponding XML files to the new folder\n",
    "  for image_file in selected_image_files:\n",
    "    src_image_path = os.path.join(original_folder, image_file)\n",
    "    dst_image_path = os.path.join(new_folder, image_file)\n",
    "    shutil.copy(src_image_path, dst_image_path)\n",
    "    # Get corresponding XML file\n",
    "    xml_file = image_file.replace('.jpg', '.xml')\n",
    "    src_xml_path = os.path.join(original_folder, xml_file)\n",
    "    dst_xml_path = os.path.join(new_folder, xml_file)\n",
    "    shutil.copy(src_xml_path, dst_xml_path)\n",
    "\n",
    "print(\"Images and XML files copied to new folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nope, no negative values or boxes with zero area. The issue now must surely be the number of images, or, if not, there's an issue with one of the images collected via Roboflow. The only thing left to do now, is to collect all the images manually. This shouldn't be a pain as the video by Nicholas Renotte has explained that a much smaller number of images than I first thought is necessary for transfer learning. Hopefully, I can cover both possible issues at the same time by doing this relatively simple task. However, still an annoyance that I can't seem to rely on people online!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL TRAINING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL TRAINING!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_SCRIPT = os.path.join(paths['APIMODEL_PATH'], 'research', 'object_detection', 'model_main_tf2.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "command = \"python {} --model_dir={} --pipeline_config_path={} --num_train_steps=5000\".format(TRAINING_SCRIPT, paths['CHECKPOINT_PATH'],files['PIPELINE_CONFIG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python /content/drive/MyDrive/Tensorflow/models/research/object_detection/model_main_tf2.py --model_dir=/content/drive/MyDrive/Tensorflow/workspace/models/topdown --pipeline_config_path=/content/drive/MyDrive/Tensorflow/workspace/models/topdown/pipeline.config --num_train_steps=5000\n"
     ]
    }
   ],
   "source": [
    "print(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-24 17:24:01.950181: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-08-24 17:24:06.185091: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "I0824 17:24:06.186330 134179689795584 mirrored_strategy.py:419] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "INFO:tensorflow:Maybe overwriting train_steps: 5000\n",
      "I0824 17:24:06.216226 134179689795584 config_util.py:552] Maybe overwriting train_steps: 5000\n",
      "INFO:tensorflow:Maybe overwriting use_bfloat16: False\n",
      "I0824 17:24:06.216558 134179689795584 config_util.py:552] Maybe overwriting use_bfloat16: False\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/object_detection/model_lib_v2.py:563: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "rename to distribute_datasets_from_function\n",
      "W0824 17:24:06.429432 134179689795584 deprecation.py:364] From /usr/local/lib/python3.10/dist-packages/object_detection/model_lib_v2.py:563: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "rename to distribute_datasets_from_function\n",
      "INFO:tensorflow:Reading unweighted datasets: ['/content/drive/MyDrive/Tensorflow/workspace/annotations/train.record']\n",
      "I0824 17:24:06.445348 134179689795584 dataset_builder.py:162] Reading unweighted datasets: ['/content/drive/MyDrive/Tensorflow/workspace/annotations/train.record']\n",
      "INFO:tensorflow:Reading record datasets for input file: ['/content/drive/MyDrive/Tensorflow/workspace/annotations/train.record']\n",
      "I0824 17:24:06.446125 134179689795584 dataset_builder.py:79] Reading record datasets for input file: ['/content/drive/MyDrive/Tensorflow/workspace/annotations/train.record']\n",
      "INFO:tensorflow:Number of filenames to read: 1\n",
      "I0824 17:24:06.446284 134179689795584 dataset_builder.py:80] Number of filenames to read: 1\n",
      "WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\n",
      "W0824 17:24:06.446343 134179689795584 dataset_builder.py:86] num_readers has been reduced to 1 to match input file shards.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/object_detection/builders/dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.\n",
      "W0824 17:24:06.454136 134179689795584 deprecation.py:364] From /usr/local/lib/python3.10/dist-packages/object_detection/builders/dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/object_detection/builders/dataset_builder.py:235: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.map()\n",
      "W0824 17:24:06.476260 134179689795584 deprecation.py:364] From /usr/local/lib/python3.10/dist-packages/object_detection/builders/dataset_builder.py:235: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.map()\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1176: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "W0824 17:24:13.573070 134179689795584 deprecation.py:364] From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1176: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1176: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n",
      "W0824 17:24:16.417564 134179689795584 deprecation.py:364] From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1176: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1176: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "W0824 17:24:18.191878 134179689795584 deprecation.py:364] From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1176: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "/usr/local/lib/python3.10/dist-packages/keras/src/backend.py:452: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
      "  warnings.warn(\n",
      "I0824 17:24:27.750555 134172513375808 api.py:460] feature_map_spatial_dims: [(40, 40), (20, 20), (10, 10), (5, 5), (3, 3)]\n",
      "I0824 17:24:37.324679 134172513375808 api.py:460] feature_map_spatial_dims: [(40, 40), (20, 20), (10, 10), (5, 5), (3, 3)]\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I0824 17:24:52.579933 134179689795584 cross_device_ops.py:617] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I0824 17:24:52.583093 134179689795584 cross_device_ops.py:617] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I0824 17:24:52.584303 134179689795584 cross_device_ops.py:617] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I0824 17:24:52.585312 134179689795584 cross_device_ops.py:617] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I0824 17:24:52.589309 134179689795584 cross_device_ops.py:617] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I0824 17:24:52.590319 134179689795584 cross_device_ops.py:617] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I0824 17:24:52.591386 134179689795584 cross_device_ops.py:617] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I0824 17:24:52.592341 134179689795584 cross_device_ops.py:617] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I0824 17:24:52.597251 134179689795584 cross_device_ops.py:617] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I0824 17:24:52.598256 134179689795584 cross_device_ops.py:617] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/deprecation.py:648: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n",
      "W0824 17:24:53.946731 134172546946624 deprecation.py:569] From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/deprecation.py:648: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n",
      "I0824 17:24:55.134340 134172546946624 api.py:460] feature_map_spatial_dims: [(40, 40), (20, 20), (10, 10), (5, 5), (3, 3)]\n",
      "I0824 17:25:01.781877 134172546946624 api.py:460] feature_map_spatial_dims: [(40, 40), (20, 20), (10, 10), (5, 5), (3, 3)]\n",
      "I0824 17:25:08.041006 134172546946624 api.py:460] feature_map_spatial_dims: [(40, 40), (20, 20), (10, 10), (5, 5), (3, 3)]\n",
      "I0824 17:25:14.478903 134172546946624 api.py:460] feature_map_spatial_dims: [(40, 40), (20, 20), (10, 10), (5, 5), (3, 3)]\n",
      "INFO:tensorflow:Step 100 per-step time 0.475s\n",
      "I0824 17:25:41.096245 134179689795584 model_lib_v2.py:705] Step 100 per-step time 0.475s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.5868803,\n",
      " 'Loss/localization_loss': 0.4465052,\n",
      " 'Loss/regularization_loss': 0.15352881,\n",
      " 'Loss/total_loss': 1.1869143,\n",
      " 'learning_rate': 0.0319994}\n",
      "I0824 17:25:41.096593 134179689795584 model_lib_v2.py:708] {'Loss/classification_loss': 0.5868803,\n",
      " 'Loss/localization_loss': 0.4465052,\n",
      " 'Loss/regularization_loss': 0.15352881,\n",
      " 'Loss/total_loss': 1.1869143,\n",
      " 'learning_rate': 0.0319994}\n",
      "INFO:tensorflow:Step 200 per-step time 0.102s\n",
      "I0824 17:25:51.238619 134179689795584 model_lib_v2.py:705] Step 200 per-step time 0.102s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.39537892,\n",
      " 'Loss/localization_loss': 0.28941685,\n",
      " 'Loss/regularization_loss': 0.15371303,\n",
      " 'Loss/total_loss': 0.8385088,\n",
      " 'learning_rate': 0.0373328}\n",
      "I0824 17:25:51.238965 134179689795584 model_lib_v2.py:708] {'Loss/classification_loss': 0.39537892,\n",
      " 'Loss/localization_loss': 0.28941685,\n",
      " 'Loss/regularization_loss': 0.15371303,\n",
      " 'Loss/total_loss': 0.8385088,\n",
      " 'learning_rate': 0.0373328}\n",
      "INFO:tensorflow:Step 300 per-step time 0.102s\n",
      "I0824 17:26:01.402130 134179689795584 model_lib_v2.py:705] Step 300 per-step time 0.102s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.48265505,\n",
      " 'Loss/localization_loss': 0.33094722,\n",
      " 'Loss/regularization_loss': 0.15391122,\n",
      " 'Loss/total_loss': 0.9675135,\n",
      " 'learning_rate': 0.0426662}\n",
      "I0824 17:26:01.402469 134179689795584 model_lib_v2.py:708] {'Loss/classification_loss': 0.48265505,\n",
      " 'Loss/localization_loss': 0.33094722,\n",
      " 'Loss/regularization_loss': 0.15391122,\n",
      " 'Loss/total_loss': 0.9675135,\n",
      " 'learning_rate': 0.0426662}\n",
      "INFO:tensorflow:Step 400 per-step time 0.103s\n",
      "I0824 17:26:11.714127 134179689795584 model_lib_v2.py:705] Step 400 per-step time 0.103s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.35500905,\n",
      " 'Loss/localization_loss': 0.18798164,\n",
      " 'Loss/regularization_loss': 0.15416022,\n",
      " 'Loss/total_loss': 0.6971509,\n",
      " 'learning_rate': 0.047999598}\n",
      "I0824 17:26:11.714438 134179689795584 model_lib_v2.py:708] {'Loss/classification_loss': 0.35500905,\n",
      " 'Loss/localization_loss': 0.18798164,\n",
      " 'Loss/regularization_loss': 0.15416022,\n",
      " 'Loss/total_loss': 0.6971509,\n",
      " 'learning_rate': 0.047999598}\n",
      "INFO:tensorflow:Step 500 per-step time 0.104s\n",
      "I0824 17:26:22.083664 134179689795584 model_lib_v2.py:705] Step 500 per-step time 0.104s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.31354696,\n",
      " 'Loss/localization_loss': 0.1692641,\n",
      " 'Loss/regularization_loss': 0.1544472,\n",
      " 'Loss/total_loss': 0.63725823,\n",
      " 'learning_rate': 0.053333}\n",
      "I0824 17:26:22.083983 134179689795584 model_lib_v2.py:708] {'Loss/classification_loss': 0.31354696,\n",
      " 'Loss/localization_loss': 0.1692641,\n",
      " 'Loss/regularization_loss': 0.1544472,\n",
      " 'Loss/total_loss': 0.63725823,\n",
      " 'learning_rate': 0.053333}\n",
      "INFO:tensorflow:Step 600 per-step time 0.105s\n",
      "I0824 17:26:32.603267 134179689795584 model_lib_v2.py:705] Step 600 per-step time 0.105s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.31297863,\n",
      " 'Loss/localization_loss': 0.20890996,\n",
      " 'Loss/regularization_loss': 0.15477046,\n",
      " 'Loss/total_loss': 0.6766591,\n",
      " 'learning_rate': 0.0586664}\n",
      "I0824 17:26:32.603562 134179689795584 model_lib_v2.py:708] {'Loss/classification_loss': 0.31297863,\n",
      " 'Loss/localization_loss': 0.20890996,\n",
      " 'Loss/regularization_loss': 0.15477046,\n",
      " 'Loss/total_loss': 0.6766591,\n",
      " 'learning_rate': 0.0586664}\n",
      "INFO:tensorflow:Step 700 per-step time 0.106s\n",
      "I0824 17:26:43.196599 134179689795584 model_lib_v2.py:705] Step 700 per-step time 0.106s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.32592398,\n",
      " 'Loss/localization_loss': 0.15402792,\n",
      " 'Loss/regularization_loss': 0.1551881,\n",
      " 'Loss/total_loss': 0.63514,\n",
      " 'learning_rate': 0.0639998}\n",
      "I0824 17:26:43.196925 134179689795584 model_lib_v2.py:708] {'Loss/classification_loss': 0.32592398,\n",
      " 'Loss/localization_loss': 0.15402792,\n",
      " 'Loss/regularization_loss': 0.1551881,\n",
      " 'Loss/total_loss': 0.63514,\n",
      " 'learning_rate': 0.0639998}\n",
      "INFO:tensorflow:Step 800 per-step time 0.106s\n",
      "I0824 17:26:53.842955 134179689795584 model_lib_v2.py:705] Step 800 per-step time 0.106s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.34647408,\n",
      " 'Loss/localization_loss': 0.24379443,\n",
      " 'Loss/regularization_loss': 0.15573496,\n",
      " 'Loss/total_loss': 0.74600345,\n",
      " 'learning_rate': 0.069333196}\n",
      "I0824 17:26:53.843230 134179689795584 model_lib_v2.py:708] {'Loss/classification_loss': 0.34647408,\n",
      " 'Loss/localization_loss': 0.24379443,\n",
      " 'Loss/regularization_loss': 0.15573496,\n",
      " 'Loss/total_loss': 0.74600345,\n",
      " 'learning_rate': 0.069333196}\n",
      "INFO:tensorflow:Step 900 per-step time 0.106s\n",
      "I0824 17:27:04.424744 134179689795584 model_lib_v2.py:705] Step 900 per-step time 0.106s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.28680718,\n",
      " 'Loss/localization_loss': 0.18261878,\n",
      " 'Loss/regularization_loss': 0.15638255,\n",
      " 'Loss/total_loss': 0.62580854,\n",
      " 'learning_rate': 0.074666604}\n",
      "I0824 17:27:04.425146 134179689795584 model_lib_v2.py:708] {'Loss/classification_loss': 0.28680718,\n",
      " 'Loss/localization_loss': 0.18261878,\n",
      " 'Loss/regularization_loss': 0.15638255,\n",
      " 'Loss/total_loss': 0.62580854,\n",
      " 'learning_rate': 0.074666604}\n",
      "INFO:tensorflow:Step 1000 per-step time 0.105s\n",
      "I0824 17:27:14.882671 134179689795584 model_lib_v2.py:705] Step 1000 per-step time 0.105s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.2543989,\n",
      " 'Loss/localization_loss': 0.18111657,\n",
      " 'Loss/regularization_loss': 0.15716939,\n",
      " 'Loss/total_loss': 0.59268486,\n",
      " 'learning_rate': 0.08}\n",
      "I0824 17:27:14.883056 134179689795584 model_lib_v2.py:708] {'Loss/classification_loss': 0.2543989,\n",
      " 'Loss/localization_loss': 0.18111657,\n",
      " 'Loss/regularization_loss': 0.15716939,\n",
      " 'Loss/total_loss': 0.59268486,\n",
      " 'learning_rate': 0.08}\n",
      "INFO:tensorflow:Step 1100 per-step time 0.117s\n",
      "I0824 17:27:26.601186 134179689795584 model_lib_v2.py:705] Step 1100 per-step time 0.117s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.2732481,\n",
      " 'Loss/localization_loss': 0.16365889,\n",
      " 'Loss/regularization_loss': 0.15785806,\n",
      " 'Loss/total_loss': 0.59476507,\n",
      " 'learning_rate': 0.07999918}\n",
      "I0824 17:27:26.601518 134179689795584 model_lib_v2.py:708] {'Loss/classification_loss': 0.2732481,\n",
      " 'Loss/localization_loss': 0.16365889,\n",
      " 'Loss/regularization_loss': 0.15785806,\n",
      " 'Loss/total_loss': 0.59476507,\n",
      " 'learning_rate': 0.07999918}\n",
      "INFO:tensorflow:Step 1200 per-step time 0.104s\n",
      "I0824 17:27:37.008619 134179689795584 model_lib_v2.py:705] Step 1200 per-step time 0.104s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.26807564,\n",
      " 'Loss/localization_loss': 0.099991046,\n",
      " 'Loss/regularization_loss': 0.15844469,\n",
      " 'Loss/total_loss': 0.5265114,\n",
      " 'learning_rate': 0.079996705}\n",
      "I0824 17:27:37.008947 134179689795584 model_lib_v2.py:708] {'Loss/classification_loss': 0.26807564,\n",
      " 'Loss/localization_loss': 0.099991046,\n",
      " 'Loss/regularization_loss': 0.15844469,\n",
      " 'Loss/total_loss': 0.5265114,\n",
      " 'learning_rate': 0.079996705}\n",
      "INFO:tensorflow:Step 1300 per-step time 0.105s\n",
      "I0824 17:27:47.468287 134179689795584 model_lib_v2.py:705] Step 1300 per-step time 0.105s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.26669383,\n",
      " 'Loss/localization_loss': 0.15745166,\n",
      " 'Loss/regularization_loss': 0.15883161,\n",
      " 'Loss/total_loss': 0.5829771,\n",
      " 'learning_rate': 0.0799926}\n",
      "I0824 17:27:47.468588 134179689795584 model_lib_v2.py:708] {'Loss/classification_loss': 0.26669383,\n",
      " 'Loss/localization_loss': 0.15745166,\n",
      " 'Loss/regularization_loss': 0.15883161,\n",
      " 'Loss/total_loss': 0.5829771,\n",
      " 'learning_rate': 0.0799926}\n",
      "INFO:tensorflow:Step 1400 per-step time 0.105s\n",
      "I0824 17:27:57.992298 134179689795584 model_lib_v2.py:705] Step 1400 per-step time 0.105s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.22036672,\n",
      " 'Loss/localization_loss': 0.11499399,\n",
      " 'Loss/regularization_loss': 0.1590415,\n",
      " 'Loss/total_loss': 0.4944022,\n",
      " 'learning_rate': 0.07998685}\n",
      "I0824 17:27:57.992591 134179689795584 model_lib_v2.py:708] {'Loss/classification_loss': 0.22036672,\n",
      " 'Loss/localization_loss': 0.11499399,\n",
      " 'Loss/regularization_loss': 0.1590415,\n",
      " 'Loss/total_loss': 0.4944022,\n",
      " 'learning_rate': 0.07998685}\n",
      "INFO:tensorflow:Step 1500 per-step time 0.106s\n",
      "I0824 17:28:08.573137 134179689795584 model_lib_v2.py:705] Step 1500 per-step time 0.106s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.22890638,\n",
      " 'Loss/localization_loss': 0.13727568,\n",
      " 'Loss/regularization_loss': 0.15922765,\n",
      " 'Loss/total_loss': 0.5254097,\n",
      " 'learning_rate': 0.07997945}\n",
      "I0824 17:28:08.573446 134179689795584 model_lib_v2.py:708] {'Loss/classification_loss': 0.22890638,\n",
      " 'Loss/localization_loss': 0.13727568,\n",
      " 'Loss/regularization_loss': 0.15922765,\n",
      " 'Loss/total_loss': 0.5254097,\n",
      " 'learning_rate': 0.07997945}\n",
      "INFO:tensorflow:Step 1600 per-step time 0.105s\n",
      "I0824 17:28:19.122189 134179689795584 model_lib_v2.py:705] Step 1600 per-step time 0.105s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.22399877,\n",
      " 'Loss/localization_loss': 0.16087621,\n",
      " 'Loss/regularization_loss': 0.15947719,\n",
      " 'Loss/total_loss': 0.5443522,\n",
      " 'learning_rate': 0.079970405}\n",
      "I0824 17:28:19.122508 134179689795584 model_lib_v2.py:708] {'Loss/classification_loss': 0.22399877,\n",
      " 'Loss/localization_loss': 0.16087621,\n",
      " 'Loss/regularization_loss': 0.15947719,\n",
      " 'Loss/total_loss': 0.5443522,\n",
      " 'learning_rate': 0.079970405}\n",
      "INFO:tensorflow:Step 1700 per-step time 0.106s\n",
      "I0824 17:28:29.683169 134179689795584 model_lib_v2.py:705] Step 1700 per-step time 0.106s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.19361435,\n",
      " 'Loss/localization_loss': 0.099026,\n",
      " 'Loss/regularization_loss': 0.15962529,\n",
      " 'Loss/total_loss': 0.45226565,\n",
      " 'learning_rate': 0.07995972}\n",
      "I0824 17:28:29.683475 134179689795584 model_lib_v2.py:708] {'Loss/classification_loss': 0.19361435,\n",
      " 'Loss/localization_loss': 0.099026,\n",
      " 'Loss/regularization_loss': 0.15962529,\n",
      " 'Loss/total_loss': 0.45226565,\n",
      " 'learning_rate': 0.07995972}\n",
      "INFO:tensorflow:Step 1800 per-step time 0.105s\n",
      "I0824 17:28:40.190910 134179689795584 model_lib_v2.py:705] Step 1800 per-step time 0.105s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.1713496,\n",
      " 'Loss/localization_loss': 0.11218899,\n",
      " 'Loss/regularization_loss': 0.1598601,\n",
      " 'Loss/total_loss': 0.44339868,\n",
      " 'learning_rate': 0.0799474}\n",
      "I0824 17:28:40.191228 134179689795584 model_lib_v2.py:708] {'Loss/classification_loss': 0.1713496,\n",
      " 'Loss/localization_loss': 0.11218899,\n",
      " 'Loss/regularization_loss': 0.1598601,\n",
      " 'Loss/total_loss': 0.44339868,\n",
      " 'learning_rate': 0.0799474}\n",
      "INFO:tensorflow:Step 1900 per-step time 0.104s\n",
      "I0824 17:28:50.623784 134179689795584 model_lib_v2.py:705] Step 1900 per-step time 0.104s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.17716691,\n",
      " 'Loss/localization_loss': 0.1205379,\n",
      " 'Loss/regularization_loss': 0.15979637,\n",
      " 'Loss/total_loss': 0.45750117,\n",
      " 'learning_rate': 0.07993342}\n",
      "I0824 17:28:50.624198 134179689795584 model_lib_v2.py:708] {'Loss/classification_loss': 0.17716691,\n",
      " 'Loss/localization_loss': 0.1205379,\n",
      " 'Loss/regularization_loss': 0.15979637,\n",
      " 'Loss/total_loss': 0.45750117,\n",
      " 'learning_rate': 0.07993342}\n",
      "INFO:tensorflow:Step 2000 per-step time 0.105s\n",
      "I0824 17:29:01.116860 134179689795584 model_lib_v2.py:705] Step 2000 per-step time 0.105s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.2117028,\n",
      " 'Loss/localization_loss': 0.140401,\n",
      " 'Loss/regularization_loss': 0.1597968,\n",
      " 'Loss/total_loss': 0.5119006,\n",
      " 'learning_rate': 0.07991781}\n",
      "I0824 17:29:01.117162 134179689795584 model_lib_v2.py:708] {'Loss/classification_loss': 0.2117028,\n",
      " 'Loss/localization_loss': 0.140401,\n",
      " 'Loss/regularization_loss': 0.1597968,\n",
      " 'Loss/total_loss': 0.5119006,\n",
      " 'learning_rate': 0.07991781}\n",
      "INFO:tensorflow:Step 2100 per-step time 0.117s\n",
      "I0824 17:29:12.807817 134179689795584 model_lib_v2.py:705] Step 2100 per-step time 0.117s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.2241092,\n",
      " 'Loss/localization_loss': 0.12042839,\n",
      " 'Loss/regularization_loss': 0.15987305,\n",
      " 'Loss/total_loss': 0.5044106,\n",
      " 'learning_rate': 0.07990056}\n",
      "I0824 17:29:12.808149 134179689795584 model_lib_v2.py:708] {'Loss/classification_loss': 0.2241092,\n",
      " 'Loss/localization_loss': 0.12042839,\n",
      " 'Loss/regularization_loss': 0.15987305,\n",
      " 'Loss/total_loss': 0.5044106,\n",
      " 'learning_rate': 0.07990056}\n",
      "INFO:tensorflow:Step 2200 per-step time 0.105s\n",
      "I0824 17:29:23.358241 134179689795584 model_lib_v2.py:705] Step 2200 per-step time 0.105s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.16081157,\n",
      " 'Loss/localization_loss': 0.124100834,\n",
      " 'Loss/regularization_loss': 0.16019398,\n",
      " 'Loss/total_loss': 0.4451064,\n",
      " 'learning_rate': 0.07988167}\n",
      "I0824 17:29:23.358696 134179689795584 model_lib_v2.py:708] {'Loss/classification_loss': 0.16081157,\n",
      " 'Loss/localization_loss': 0.124100834,\n",
      " 'Loss/regularization_loss': 0.16019398,\n",
      " 'Loss/total_loss': 0.4451064,\n",
      " 'learning_rate': 0.07988167}\n",
      "INFO:tensorflow:Step 2300 per-step time 0.106s\n",
      "I0824 17:29:33.899902 134179689795584 model_lib_v2.py:705] Step 2300 per-step time 0.106s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.14936593,\n",
      " 'Loss/localization_loss': 0.10644867,\n",
      " 'Loss/regularization_loss': 0.16006362,\n",
      " 'Loss/total_loss': 0.41587824,\n",
      " 'learning_rate': 0.07986114}\n",
      "I0824 17:29:33.900194 134179689795584 model_lib_v2.py:708] {'Loss/classification_loss': 0.14936593,\n",
      " 'Loss/localization_loss': 0.10644867,\n",
      " 'Loss/regularization_loss': 0.16006362,\n",
      " 'Loss/total_loss': 0.41587824,\n",
      " 'learning_rate': 0.07986114}\n",
      "INFO:tensorflow:Step 2400 per-step time 0.105s\n",
      "I0824 17:29:44.375262 134179689795584 model_lib_v2.py:705] Step 2400 per-step time 0.105s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.12710635,\n",
      " 'Loss/localization_loss': 0.07238627,\n",
      " 'Loss/regularization_loss': 0.15990616,\n",
      " 'Loss/total_loss': 0.35939878,\n",
      " 'learning_rate': 0.07983897}\n",
      "I0824 17:29:44.375558 134179689795584 model_lib_v2.py:708] {'Loss/classification_loss': 0.12710635,\n",
      " 'Loss/localization_loss': 0.07238627,\n",
      " 'Loss/regularization_loss': 0.15990616,\n",
      " 'Loss/total_loss': 0.35939878,\n",
      " 'learning_rate': 0.07983897}\n",
      "INFO:tensorflow:Step 2500 per-step time 0.106s\n",
      "I0824 17:29:54.943329 134179689795584 model_lib_v2.py:705] Step 2500 per-step time 0.106s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.18504977,\n",
      " 'Loss/localization_loss': 0.09765757,\n",
      " 'Loss/regularization_loss': 0.15974464,\n",
      " 'Loss/total_loss': 0.44245195,\n",
      " 'learning_rate': 0.079815164}\n",
      "I0824 17:29:54.943624 134179689795584 model_lib_v2.py:708] {'Loss/classification_loss': 0.18504977,\n",
      " 'Loss/localization_loss': 0.09765757,\n",
      " 'Loss/regularization_loss': 0.15974464,\n",
      " 'Loss/total_loss': 0.44245195,\n",
      " 'learning_rate': 0.079815164}\n",
      "INFO:tensorflow:Step 2600 per-step time 0.105s\n",
      "I0824 17:30:05.408501 134179689795584 model_lib_v2.py:705] Step 2600 per-step time 0.105s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.16068605,\n",
      " 'Loss/localization_loss': 0.095386334,\n",
      " 'Loss/regularization_loss': 0.15964763,\n",
      " 'Loss/total_loss': 0.41572,\n",
      " 'learning_rate': 0.07978972}\n",
      "I0824 17:30:05.408804 134179689795584 model_lib_v2.py:708] {'Loss/classification_loss': 0.16068605,\n",
      " 'Loss/localization_loss': 0.095386334,\n",
      " 'Loss/regularization_loss': 0.15964763,\n",
      " 'Loss/total_loss': 0.41572,\n",
      " 'learning_rate': 0.07978972}\n",
      "INFO:tensorflow:Step 2700 per-step time 0.105s\n",
      "I0824 17:30:15.916533 134179689795584 model_lib_v2.py:705] Step 2700 per-step time 0.105s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.13989171,\n",
      " 'Loss/localization_loss': 0.060588922,\n",
      " 'Loss/regularization_loss': 0.15927544,\n",
      " 'Loss/total_loss': 0.35975608,\n",
      " 'learning_rate': 0.07976264}\n",
      "I0824 17:30:15.916826 134179689795584 model_lib_v2.py:708] {'Loss/classification_loss': 0.13989171,\n",
      " 'Loss/localization_loss': 0.060588922,\n",
      " 'Loss/regularization_loss': 0.15927544,\n",
      " 'Loss/total_loss': 0.35975608,\n",
      " 'learning_rate': 0.07976264}\n",
      "INFO:tensorflow:Step 2800 per-step time 0.105s\n",
      "I0824 17:30:26.463129 134179689795584 model_lib_v2.py:705] Step 2800 per-step time 0.105s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.17144631,\n",
      " 'Loss/localization_loss': 0.11743016,\n",
      " 'Loss/regularization_loss': 0.15904349,\n",
      " 'Loss/total_loss': 0.44791996,\n",
      " 'learning_rate': 0.07973392}\n",
      "I0824 17:30:26.463494 134179689795584 model_lib_v2.py:708] {'Loss/classification_loss': 0.17144631,\n",
      " 'Loss/localization_loss': 0.11743016,\n",
      " 'Loss/regularization_loss': 0.15904349,\n",
      " 'Loss/total_loss': 0.44791996,\n",
      " 'learning_rate': 0.07973392}\n",
      "INFO:tensorflow:Step 2900 per-step time 0.105s\n",
      "I0824 17:30:36.985721 134179689795584 model_lib_v2.py:705] Step 2900 per-step time 0.105s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.14388861,\n",
      " 'Loss/localization_loss': 0.08488535,\n",
      " 'Loss/regularization_loss': 0.15872945,\n",
      " 'Loss/total_loss': 0.3875034,\n",
      " 'learning_rate': 0.07970358}\n",
      "I0824 17:30:36.986067 134179689795584 model_lib_v2.py:708] {'Loss/classification_loss': 0.14388861,\n",
      " 'Loss/localization_loss': 0.08488535,\n",
      " 'Loss/regularization_loss': 0.15872945,\n",
      " 'Loss/total_loss': 0.3875034,\n",
      " 'learning_rate': 0.07970358}\n",
      "INFO:tensorflow:Step 3000 per-step time 0.105s\n",
      "I0824 17:30:47.481874 134179689795584 model_lib_v2.py:705] Step 3000 per-step time 0.105s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.1142262,\n",
      " 'Loss/localization_loss': 0.06461565,\n",
      " 'Loss/regularization_loss': 0.15850082,\n",
      " 'Loss/total_loss': 0.33734268,\n",
      " 'learning_rate': 0.0796716}\n",
      "I0824 17:30:47.482395 134179689795584 model_lib_v2.py:708] {'Loss/classification_loss': 0.1142262,\n",
      " 'Loss/localization_loss': 0.06461565,\n",
      " 'Loss/regularization_loss': 0.15850082,\n",
      " 'Loss/total_loss': 0.33734268,\n",
      " 'learning_rate': 0.0796716}\n",
      "INFO:tensorflow:Step 3100 per-step time 0.117s\n",
      "I0824 17:30:59.153512 134179689795584 model_lib_v2.py:705] Step 3100 per-step time 0.117s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.13448969,\n",
      " 'Loss/localization_loss': 0.07995278,\n",
      " 'Loss/regularization_loss': 0.15820168,\n",
      " 'Loss/total_loss': 0.37264413,\n",
      " 'learning_rate': 0.07963799}\n",
      "I0824 17:30:59.153832 134179689795584 model_lib_v2.py:708] {'Loss/classification_loss': 0.13448969,\n",
      " 'Loss/localization_loss': 0.07995278,\n",
      " 'Loss/regularization_loss': 0.15820168,\n",
      " 'Loss/total_loss': 0.37264413,\n",
      " 'learning_rate': 0.07963799}\n",
      "INFO:tensorflow:Step 3200 per-step time 0.105s\n",
      "I0824 17:31:09.698145 134179689795584 model_lib_v2.py:705] Step 3200 per-step time 0.105s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.14968687,\n",
      " 'Loss/localization_loss': 0.117559366,\n",
      " 'Loss/regularization_loss': 0.15787031,\n",
      " 'Loss/total_loss': 0.42511654,\n",
      " 'learning_rate': 0.07960275}\n",
      "I0824 17:31:09.698495 134179689795584 model_lib_v2.py:708] {'Loss/classification_loss': 0.14968687,\n",
      " 'Loss/localization_loss': 0.117559366,\n",
      " 'Loss/regularization_loss': 0.15787031,\n",
      " 'Loss/total_loss': 0.42511654,\n",
      " 'learning_rate': 0.07960275}\n",
      "INFO:tensorflow:Step 3300 per-step time 0.105s\n",
      "I0824 17:31:20.203908 134179689795584 model_lib_v2.py:705] Step 3300 per-step time 0.105s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.15739547,\n",
      " 'Loss/localization_loss': 0.1087625,\n",
      " 'Loss/regularization_loss': 0.15749983,\n",
      " 'Loss/total_loss': 0.42365783,\n",
      " 'learning_rate': 0.07956588}\n",
      "I0824 17:31:20.204229 134179689795584 model_lib_v2.py:708] {'Loss/classification_loss': 0.15739547,\n",
      " 'Loss/localization_loss': 0.1087625,\n",
      " 'Loss/regularization_loss': 0.15749983,\n",
      " 'Loss/total_loss': 0.42365783,\n",
      " 'learning_rate': 0.07956588}\n",
      "INFO:tensorflow:Step 3400 per-step time 0.105s\n",
      "I0824 17:31:30.735889 134179689795584 model_lib_v2.py:705] Step 3400 per-step time 0.105s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.10083801,\n",
      " 'Loss/localization_loss': 0.05055547,\n",
      " 'Loss/regularization_loss': 0.15723497,\n",
      " 'Loss/total_loss': 0.30862844,\n",
      " 'learning_rate': 0.079527386}\n",
      "I0824 17:31:30.736254 134179689795584 model_lib_v2.py:708] {'Loss/classification_loss': 0.10083801,\n",
      " 'Loss/localization_loss': 0.05055547,\n",
      " 'Loss/regularization_loss': 0.15723497,\n",
      " 'Loss/total_loss': 0.30862844,\n",
      " 'learning_rate': 0.079527386}\n",
      "INFO:tensorflow:Step 3500 per-step time 0.105s\n",
      "I0824 17:31:41.243784 134179689795584 model_lib_v2.py:705] Step 3500 per-step time 0.105s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.13793524,\n",
      " 'Loss/localization_loss': 0.082247406,\n",
      " 'Loss/regularization_loss': 0.15674564,\n",
      " 'Loss/total_loss': 0.37692827,\n",
      " 'learning_rate': 0.07948727}\n",
      "I0824 17:31:41.244116 134179689795584 model_lib_v2.py:708] {'Loss/classification_loss': 0.13793524,\n",
      " 'Loss/localization_loss': 0.082247406,\n",
      " 'Loss/regularization_loss': 0.15674564,\n",
      " 'Loss/total_loss': 0.37692827,\n",
      " 'learning_rate': 0.07948727}\n",
      "INFO:tensorflow:Step 3600 per-step time 0.105s\n",
      "I0824 17:31:51.704778 134179689795584 model_lib_v2.py:705] Step 3600 per-step time 0.105s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.0950741,\n",
      " 'Loss/localization_loss': 0.054889046,\n",
      " 'Loss/regularization_loss': 0.15643585,\n",
      " 'Loss/total_loss': 0.306399,\n",
      " 'learning_rate': 0.079445526}\n",
      "I0824 17:31:51.705113 134179689795584 model_lib_v2.py:708] {'Loss/classification_loss': 0.0950741,\n",
      " 'Loss/localization_loss': 0.054889046,\n",
      " 'Loss/regularization_loss': 0.15643585,\n",
      " 'Loss/total_loss': 0.306399,\n",
      " 'learning_rate': 0.079445526}\n",
      "INFO:tensorflow:Step 3700 per-step time 0.105s\n",
      "I0824 17:32:02.204487 134179689795584 model_lib_v2.py:705] Step 3700 per-step time 0.105s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.15255363,\n",
      " 'Loss/localization_loss': 0.09999674,\n",
      " 'Loss/regularization_loss': 0.15611139,\n",
      " 'Loss/total_loss': 0.40866175,\n",
      " 'learning_rate': 0.07940216}\n",
      "I0824 17:32:02.204787 134179689795584 model_lib_v2.py:708] {'Loss/classification_loss': 0.15255363,\n",
      " 'Loss/localization_loss': 0.09999674,\n",
      " 'Loss/regularization_loss': 0.15611139,\n",
      " 'Loss/total_loss': 0.40866175,\n",
      " 'learning_rate': 0.07940216}\n",
      "INFO:tensorflow:Step 3800 per-step time 0.105s\n",
      "I0824 17:32:12.696289 134179689795584 model_lib_v2.py:705] Step 3800 per-step time 0.105s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.11927149,\n",
      " 'Loss/localization_loss': 0.06078475,\n",
      " 'Loss/regularization_loss': 0.15569443,\n",
      " 'Loss/total_loss': 0.33575067,\n",
      " 'learning_rate': 0.079357184}\n",
      "I0824 17:32:12.696603 134179689795584 model_lib_v2.py:708] {'Loss/classification_loss': 0.11927149,\n",
      " 'Loss/localization_loss': 0.06078475,\n",
      " 'Loss/regularization_loss': 0.15569443,\n",
      " 'Loss/total_loss': 0.33575067,\n",
      " 'learning_rate': 0.079357184}\n",
      "INFO:tensorflow:Step 3900 per-step time 0.106s\n",
      "I0824 17:32:23.271228 134179689795584 model_lib_v2.py:705] Step 3900 per-step time 0.106s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.12121828,\n",
      " 'Loss/localization_loss': 0.054438367,\n",
      " 'Loss/regularization_loss': 0.15533486,\n",
      " 'Loss/total_loss': 0.3309915,\n",
      " 'learning_rate': 0.07931058}\n",
      "I0824 17:32:23.271585 134179689795584 model_lib_v2.py:708] {'Loss/classification_loss': 0.12121828,\n",
      " 'Loss/localization_loss': 0.054438367,\n",
      " 'Loss/regularization_loss': 0.15533486,\n",
      " 'Loss/total_loss': 0.3309915,\n",
      " 'learning_rate': 0.07931058}\n",
      "INFO:tensorflow:Step 4000 per-step time 0.104s\n",
      "I0824 17:32:33.691909 134179689795584 model_lib_v2.py:705] Step 4000 per-step time 0.104s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.1227143,\n",
      " 'Loss/localization_loss': 0.06745332,\n",
      " 'Loss/regularization_loss': 0.15513234,\n",
      " 'Loss/total_loss': 0.34529996,\n",
      " 'learning_rate': 0.07926236}\n",
      "I0824 17:32:33.692339 134179689795584 model_lib_v2.py:708] {'Loss/classification_loss': 0.1227143,\n",
      " 'Loss/localization_loss': 0.06745332,\n",
      " 'Loss/regularization_loss': 0.15513234,\n",
      " 'Loss/total_loss': 0.34529996,\n",
      " 'learning_rate': 0.07926236}\n",
      "INFO:tensorflow:Step 4100 per-step time 0.117s\n",
      "I0824 17:32:45.428035 134179689795584 model_lib_v2.py:705] Step 4100 per-step time 0.117s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.13797401,\n",
      " 'Loss/localization_loss': 0.05566762,\n",
      " 'Loss/regularization_loss': 0.1553656,\n",
      " 'Loss/total_loss': 0.34900725,\n",
      " 'learning_rate': 0.07921253}\n",
      "I0824 17:32:45.428481 134179689795584 model_lib_v2.py:708] {'Loss/classification_loss': 0.13797401,\n",
      " 'Loss/localization_loss': 0.05566762,\n",
      " 'Loss/regularization_loss': 0.1553656,\n",
      " 'Loss/total_loss': 0.34900725,\n",
      " 'learning_rate': 0.07921253}\n",
      "INFO:tensorflow:Step 4200 per-step time 0.105s\n",
      "I0824 17:32:55.964480 134179689795584 model_lib_v2.py:705] Step 4200 per-step time 0.105s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.13215992,\n",
      " 'Loss/localization_loss': 0.06378342,\n",
      " 'Loss/regularization_loss': 0.15539397,\n",
      " 'Loss/total_loss': 0.3513373,\n",
      " 'learning_rate': 0.07916109}\n",
      "I0824 17:32:55.964774 134179689795584 model_lib_v2.py:708] {'Loss/classification_loss': 0.13215992,\n",
      " 'Loss/localization_loss': 0.06378342,\n",
      " 'Loss/regularization_loss': 0.15539397,\n",
      " 'Loss/total_loss': 0.3513373,\n",
      " 'learning_rate': 0.07916109}\n",
      "INFO:tensorflow:Step 4300 per-step time 0.105s\n",
      "I0824 17:33:06.485346 134179689795584 model_lib_v2.py:705] Step 4300 per-step time 0.105s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.113004066,\n",
      " 'Loss/localization_loss': 0.05539565,\n",
      " 'Loss/regularization_loss': 0.15500268,\n",
      " 'Loss/total_loss': 0.3234024,\n",
      " 'learning_rate': 0.07910804}\n",
      "I0824 17:33:06.485697 134179689795584 model_lib_v2.py:708] {'Loss/classification_loss': 0.113004066,\n",
      " 'Loss/localization_loss': 0.05539565,\n",
      " 'Loss/regularization_loss': 0.15500268,\n",
      " 'Loss/total_loss': 0.3234024,\n",
      " 'learning_rate': 0.07910804}\n",
      "INFO:tensorflow:Step 4400 per-step time 0.106s\n",
      "I0824 17:33:17.039146 134179689795584 model_lib_v2.py:705] Step 4400 per-step time 0.106s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.13024762,\n",
      " 'Loss/localization_loss': 0.094032906,\n",
      " 'Loss/regularization_loss': 0.1545573,\n",
      " 'Loss/total_loss': 0.37883782,\n",
      " 'learning_rate': 0.07905338}\n",
      "I0824 17:33:17.039433 134179689795584 model_lib_v2.py:708] {'Loss/classification_loss': 0.13024762,\n",
      " 'Loss/localization_loss': 0.094032906,\n",
      " 'Loss/regularization_loss': 0.1545573,\n",
      " 'Loss/total_loss': 0.37883782,\n",
      " 'learning_rate': 0.07905338}\n",
      "INFO:tensorflow:Step 4500 per-step time 0.105s\n",
      "I0824 17:33:27.521814 134179689795584 model_lib_v2.py:705] Step 4500 per-step time 0.105s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.11085263,\n",
      " 'Loss/localization_loss': 0.064654484,\n",
      " 'Loss/regularization_loss': 0.15414388,\n",
      " 'Loss/total_loss': 0.329651,\n",
      " 'learning_rate': 0.07899711}\n",
      "I0824 17:33:27.522143 134179689795584 model_lib_v2.py:708] {'Loss/classification_loss': 0.11085263,\n",
      " 'Loss/localization_loss': 0.064654484,\n",
      " 'Loss/regularization_loss': 0.15414388,\n",
      " 'Loss/total_loss': 0.329651,\n",
      " 'learning_rate': 0.07899711}\n",
      "INFO:tensorflow:Step 4600 per-step time 0.106s\n",
      "I0824 17:33:38.098396 134179689795584 model_lib_v2.py:705] Step 4600 per-step time 0.106s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.12609483,\n",
      " 'Loss/localization_loss': 0.052838575,\n",
      " 'Loss/regularization_loss': 0.1536874,\n",
      " 'Loss/total_loss': 0.3326208,\n",
      " 'learning_rate': 0.078939244}\n",
      "I0824 17:33:38.098692 134179689795584 model_lib_v2.py:708] {'Loss/classification_loss': 0.12609483,\n",
      " 'Loss/localization_loss': 0.052838575,\n",
      " 'Loss/regularization_loss': 0.1536874,\n",
      " 'Loss/total_loss': 0.3326208,\n",
      " 'learning_rate': 0.078939244}\n",
      "INFO:tensorflow:Step 4700 per-step time 0.105s\n",
      "I0824 17:33:48.557127 134179689795584 model_lib_v2.py:705] Step 4700 per-step time 0.105s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.096225604,\n",
      " 'Loss/localization_loss': 0.06748908,\n",
      " 'Loss/regularization_loss': 0.15318744,\n",
      " 'Loss/total_loss': 0.3169021,\n",
      " 'learning_rate': 0.07887978}\n",
      "I0824 17:33:48.557538 134179689795584 model_lib_v2.py:708] {'Loss/classification_loss': 0.096225604,\n",
      " 'Loss/localization_loss': 0.06748908,\n",
      " 'Loss/regularization_loss': 0.15318744,\n",
      " 'Loss/total_loss': 0.3169021,\n",
      " 'learning_rate': 0.07887978}\n",
      "INFO:tensorflow:Step 4800 per-step time 0.105s\n",
      "I0824 17:33:59.031578 134179689795584 model_lib_v2.py:705] Step 4800 per-step time 0.105s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.09956065,\n",
      " 'Loss/localization_loss': 0.071127035,\n",
      " 'Loss/regularization_loss': 0.15270384,\n",
      " 'Loss/total_loss': 0.3233915,\n",
      " 'learning_rate': 0.07881871}\n",
      "I0824 17:33:59.031879 134179689795584 model_lib_v2.py:708] {'Loss/classification_loss': 0.09956065,\n",
      " 'Loss/localization_loss': 0.071127035,\n",
      " 'Loss/regularization_loss': 0.15270384,\n",
      " 'Loss/total_loss': 0.3233915,\n",
      " 'learning_rate': 0.07881871}\n",
      "INFO:tensorflow:Step 4900 per-step time 0.105s\n",
      "I0824 17:34:09.521742 134179689795584 model_lib_v2.py:705] Step 4900 per-step time 0.105s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.11630253,\n",
      " 'Loss/localization_loss': 0.063082315,\n",
      " 'Loss/regularization_loss': 0.15225823,\n",
      " 'Loss/total_loss': 0.33164307,\n",
      " 'learning_rate': 0.07875605}\n",
      "I0824 17:34:09.522223 134179689795584 model_lib_v2.py:708] {'Loss/classification_loss': 0.11630253,\n",
      " 'Loss/localization_loss': 0.063082315,\n",
      " 'Loss/regularization_loss': 0.15225823,\n",
      " 'Loss/total_loss': 0.33164307,\n",
      " 'learning_rate': 0.07875605}\n",
      "INFO:tensorflow:Step 5000 per-step time 0.105s\n",
      "I0824 17:34:20.023752 134179689795584 model_lib_v2.py:705] Step 5000 per-step time 0.105s\n",
      "INFO:tensorflow:{'Loss/classification_loss': 0.106392905,\n",
      " 'Loss/localization_loss': 0.084165566,\n",
      " 'Loss/regularization_loss': 0.15181385,\n",
      " 'Loss/total_loss': 0.3423723,\n",
      " 'learning_rate': 0.078691795}\n",
      "I0824 17:34:20.024099 134179689795584 model_lib_v2.py:708] {'Loss/classification_loss': 0.106392905,\n",
      " 'Loss/localization_loss': 0.084165566,\n",
      " 'Loss/regularization_loss': 0.15181385,\n",
      " 'Loss/total_loss': 0.3423723,\n",
      " 'learning_rate': 0.078691795}\n"
     ]
    }
   ],
   "source": [
    "!{command}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REFLECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reflection of pre-processing and finally getting model to train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, after completing several pre-processing steps, the model can finally train with a subset of images: 10 per class for training, and 2 per class for testing.\n",
    "I also had to:\n",
    "* change the generate_tfrecord.py script in **'Create TFRecords'**, to compensate for the unexpected change in order displayed in .xml bounding box co-ordinates. This should be fine now that I have specifically assigned the values xmin, ymin etc. to the names which appear in .xml classes, and not based on their position... The change in position meant that some values were wrongly assigned which resulted in negative areas.\n",
    "* convert some images to JPEG format in **'Processing the pixel values to help image processing'**. These images in question were collected by me using Google, and thought none the wiser as to how formats are so deadly to Tensorflow image processing modules. It turns out a WEBP image format is not supported, so this resulted in having to convert each image to JPEG, if it wasn't in this format already. These images were collected in a smaller subset along with relevant .xml files, as this issue seemed to be a memory usage issue at the time; this is not so, at least for about 100 images.\n",
    "* delete any images in the pool that didn't have an assigned .xml file in **'.XML FILES DEBUGGING'**.\n",
    "                 \\/ \\/ \\/ \\/ \\/ \\/ \\/ \\/ \\/ \\/ \\/ \\/ \\/ \\/    \n",
    "* and finally, <u>**creating multiple backups of my images**</u> <-- very important.\n",
    "\n",
    "                 ^^ ^^ ^^ ^^ ^^ ^^ ^^ ^^ ^^ ^^ ^^ ^^ ^^\n",
    "\n",
    "P.S. My task now is to create a precise model now, while remembering I have changed certain values in the pipeline.config file, to reflect the smaller subset of data."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
